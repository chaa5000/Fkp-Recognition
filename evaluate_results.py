import numpy as np
from sklearn.metrics import roc_curve, auc

def evaluate_results(results, decision_mode='ID', results1=None):
    """
    Evaluates the results produced by the nearest neighbor classifier.

    Args:
        results (dict): A result structure generated by the `nn_classification` function.
        decision_mode (str): Decision mode ('ID' or 'image'). Defaults to 'ID'.
        results1 (dict, optional): Another result structure for evaluation. Defaults to None.

    Returns:
        dict: Evaluation metrics including ROC and CMC curves, EER, and others.
    """
    output = {}

    # Validate inputs
    if decision_mode not in ['ID', 'image']:
        raise ValueError("The 'decision_mode' parameter must be either 'ID' or 'image'.")

    # Extract similarity matrix and IDs
    match_dist = results['match_dist']
    train_ids = results['horizontal_ids']
    test_ids = results['vertical_ids']

    # Compute client and impostor distances
    client_distances = []
    impostor_distances = []

    for i, test_id in enumerate(test_ids):
        for j, train_id in enumerate(train_ids):
            if test_id == train_id:
                client_distances.append(match_dist[i, j])
            else:
                impostor_distances.append(match_dist[i, j])

    client_distances = np.array(client_distances)
    impostor_distances = np.array(impostor_distances)

    # ROC Curve
    all_distances = np.concatenate([client_distances, impostor_distances])
    labels = np.concatenate([np.ones_like(client_distances), np.zeros_like(impostor_distances)])
    fpr, tpr, thresholds = roc_curve(labels, -all_distances)
    roc_auc = auc(fpr, tpr)

    # Compute Equal Error Rate (EER)
    fnr = 1 - tpr
    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]
    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]

    # Save ROC results
    output['ROC_ver_rate'] = tpr
    output['ROC_miss_rate'] = fpr
    output['ROC_auc'] = roc_auc
    output['EER'] = eer
    output['EER_threshold'] = eer_threshold

    # CMC Curve
    ranks = []
    for i, test_id in enumerate(test_ids):
        sorted_indices = np.argsort(match_dist[i, :])
        sorted_train_ids = train_ids[sorted_indices]
        rank = np.where(sorted_train_ids == test_id)[0][0] + 1
        ranks.append(rank)

    cmc = np.cumsum(np.histogram(ranks, bins=np.arange(1, len(train_ids) + 2))[0]) / len(ranks)
    output['CMC_rec_rates'] = cmc
    output['CMC_ranks'] = np.arange(1, len(cmc) + 1)

    return output

# Example usage:
# Example results dictionary
results = {
    'match_dist': np.random.rand(10, 20),  # Example similarity matrix (10 test samples, 20 train samples)
    'horizontal_ids': np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]),
    'vertical_ids': np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
}

# Evaluate results
output = evaluate_results(results, decision_mode='ID')

# Print evaluation metrics
print(f"ROC AUC: {output['ROC_auc']}")
print(f"EER: {output['EER'] * 100:.2f}%")
print(f"CMC Rank-1 Rate: {output['CMC_rec_rates'][0] * 100:.2f}%")
